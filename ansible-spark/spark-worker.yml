---
- name: Ansible for Spark Installation
  hosts: spark_workers
  gather_facts: no # to turn off setup
  pre_tasks:
    - name: 'install python2'
      raw: sudo apt-get -y install python-simplejson
  tasks:
    - name: Add public DNS and IP in /etc/hosts
      become: yes
      become_method: sudo
      shell: "sed -i '1i{{ ansible_host }} {{ inventory_hostname }}' /etc/hosts"
    - name: Change hostname of EC2 instance
      become: yes
      become_method: sudo
      shell: "hostname {{ ansible_host }}"
    - name: add java apt repository
      become: yes
      become_method: sudo
      shell: "{{ item }}"
      with_items: 
        - apt-add-repository ppa:webupd8team/java
        - apt-get update
    - name: Accept Java 8 License
      become: yes
      debconf: name='oracle-java8-installer' question='shared/accepted-oracle-license-v1-1' value='true' vtype='select'
    - name: install java and others
      become: yes
      become_method: sudo
      apt: name={{ item }} update_cache=yes state=latest
      with_items: 
        - oracle-java8-installer
        - ca-certificates
        - oracle-java8-set-default
        - unzip
    - name: Get Spark directory existence
      stat: path=/usr/local/spark-2.1.1-bin-hadoop2.7
      register: spark_dir
    - name: download spark from mirror and extract
      become: yes
      become_method: sudo
      when: spark_dir.stat.isdir is not defined
      shell: "{{ item }}"
      with_items: 
        - wget http://www-eu.apache.org/dist/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz 
        - tar -zxvf spark-2.1.1-bin-h\adoop2.7.tgz
        - mv spark-2.1.1-bin-hadoop2.7 /usr/local
    - name: Copy Spark configuration
      become: yes
      become_method: sudo
      copy: src={{ item.src }} dest={{ item.dest }}
      with_items:
        - { src: 'spark-env.sh', dest: '/usr/local/spark-2.1.1-bin-hadoop2.7/conf/spark-env.sh' }
    - name: Creates spark history server directory
      file: path=/tmp/spark-events state=directory
    - name: start Spark slave
      become: yes
      become_method: sudo
      shell: "{{ item }}"
      with_items: 
        - /usr/local/spark-2.1.1-bin-hadoop2.7/sbin/start-slave.sh spark://188.166.255.13:7077 -m 1G
    - name: start Spark history server
      become: yes
      become_method: sudo
      shell: "{{ item }}"
      with_items: 
        - /usr/local/spark-2.1.1-bin-hadoop2.7/sbin/start-history-server.sh
    - name: Copy Spark JAR Executable
      become: yes
      become_method: sudo
      copy: src={{ item.src }} dest={{ item.dest }}
      with_items:
        - { src: 'ITBFinalProject-assembly-1.0-pregel-deserialized.jar', dest: '/root/ITBFinalProject-assembly-1.0-pregel-deserialized.jar' }
        - { src: 'ITBFinalProject-assembly-1.0-pregel-serialized-mem.jar', dest: '/root/ITBFinalProject-assembly-1.0-pregel-serialized-mem.jar' }
        - { src: 'ITBFinalProject-assembly-1.0-pregel-deserialized-mem.jar', dest: '/root/ITBFinalProject-assembly-1.0-pregel-deserialized-mem.jar' }
        - { src: 'ITBFinalProject-assembly-1.0-pregel.jar', dest: '/root/ITBFinalProject-assembly-1.0-pregel.jar' }
